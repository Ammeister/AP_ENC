noms_fichiers <- dir()
mon_corpus <- lapply(noms_fichiers,readLines)
mon_corpus
mon_texte <- unlist(mon_corpus)
calcul_frequences <- function(mon_texte, n = mon_texte){
ma_liste_de_mots <- unlist(strsplit(mon_texte," "))
montexte_mots_freq <- summary(factor(ma_liste_de_mots))[1:n]
df <- data.frame(mots=names(montexte_mots_freq),
freq=montexte_mots_freq)
return(df)
}
calcul_frequences(mon_texte)
calcul_frequences(mon_texte)
noms_fichiers <- dir()
mon_corpus <- lapply(noms_fichiers,readLines)
mon_corpus
mon_texte <- unlist(mon_corpus)
calcul_frequences <- function(mon_texte, n = mon_texte){
ma_liste_de_mots <- unlist(strsplit(mon_texte," "))
montexte_mots_freq <- summary(factor(ma_liste_de_mots))[1:n]
df <- data.frame(mots=names(montexte_mots_freq),
freq=montexte_mots_freq)
return(df)
}
calcul_frequences(mon_texte)
library(igraph)
install.packages("FactoMineR")
# setwd("~/Data/Cours/Cours-ENC-2021-22/cours/Philologie_computationnelle_M2HN/TP_stylo_01_non-superv")
theatre = as.data.frame(read.csv(file="corpus_theatre_17/Frequences.csv", sep = ";", header = TRUE, row.names=1, quote = '\"'))
#typeof(theatre)
#class(theatre)
#theatre = as.matrix(theatre)
#View(theatre)
#Fréquences relatives
theatreTraite = theatre
for(i in 1:ncol(theatreTraite)){
theatreTraite[,i] = theatreTraite[,i]/sum(theatreTraite[,i])
}
#N plus fréquents
#Faire varier le chiffre en dessous pour tenter d'autres sélections
theatreTraite = theatreTraite[1:100,]
colnames(theatreTraite)
theatreTraite = theatreTraite[,c(5:8, 17:20, 29:32)]
#On charge FactoMineR
library('FactoMineR')
#On charge FactoMineR
library('FactoMineR')
install.packages("FactoMineR")
#On charge FactoMineR
library('FactoMineR')
monACP$eig
library("FactoMineR")
installed.packages()
library("FactoMineR")
install.packages("FactoMineR")
#setwd("~/Data/Cours/Cours-ENC-2019-20/Cours/Philologie_computationnelle_M2HN/TP_Stylometrie")
#Avec sélection
theatre2 = as.matrix(read.csv(file="corpus_theatre_17/Frequences.csv", sep = ";", header = TRUE, row.names=1, quote = '\"'))
#View(theatre2)
#View(theatre2)
rel_freqs = theatre2
#Fréquences relatives
for(i in 1:ncol(rel_freqs)){
rel_freqs[,i] = rel_freqs[,i]/sum(rel_freqs[,i])
#essai d'une normalisation logarithmique
#rel_freqs[,i] = log10(1 + rel_freqs[,i]/sum(rel_freqs[,i]))
}
# Chargeons des fonctions extérieures
source("functions.R")
data = affs # rel_freqs[1:1000,] # faire varier cela pour changer le jeu de données
install.packages("kohonen")
# Chargeons des fonctions extérieures
source("functions.R")
install.packages("ggfortify")
# Chargeons des fonctions extérieures
source("functions.R")
# et utilisons là
# Extraction des affixes
affs = countAffixes(theatre2)
affs = relativeFreqs(affs)
# Mots-outils
# Récupérons une liste préexistante de mots-outils depuis un txt cette fois
functionWords = scan("function_words.txt", what = "character")
# Et on les sélectionne
fw = rel_freqs[rownames(rel_freqs) %in% functionWords,]
data = affs # rel_freqs[1:1000,] # faire varier cela pour changer le jeu de données
#data = fw
#data = rel_freqs[1:500,]
# Là encore, une fonction pour le faire
data = normalisations(data)
#J'importe une bibliothèque de partitionnement
library(cluster)
maCAH = agnes(t(data), metric = "manhattan", method="ward")
plot(maCAH, which.plots = 2)
# Et une fonction un peu plus esthétique
cahPlotCol(maCAH, k = 9)
library(textreuse)
corpus = TextReuseCorpus(dir = "epique_medieval/txt_lemmat/", meta = list(title = "Chansons de geste"), tokenizer = tokenize_ngrams, n = 3)
comps = pairwise_compare(corpus, jaccard_similarity)
View(comps)
View(comps)
View(comps)
View(comps)
d = read.csv(file = "epique_medieval/tm_samples.tsv", sep = "\t", quote = '\"', header = TRUE)
comps = pairwise_compare(corpus, jaccard_similarity)
corpus = TextReuseCorpus(text = as.character(d[, 4]), meta = list(title = "Chansons de geste"), tokenizer = tokenize_ngrams, n = 3)
comps = pairwise_compare(corpus, jaccard_similarity)
minhash <- minhash_generator(n = 240, seed = 1991)
minhash <- minhash_generator(n = 240, seed = 1991)
corpus_mh = TextReuseCorpus(text = as.character(d[, 4]), meta = list(title = "Chansons de geste"), tokenizer = tokenize_skip_ngrams, n = 3, k = 1, minhash_func = minhash)
lsh_threshold(h = 240, b = 80)
lsh_probability(h = 240, b = 80, s = 0.6)
buckets = lsh(corpus_mh, bands = 80)
buckets = lsh(corpus_mh, bands = 80)
candidates = lsh_candidates(buckets)
comps = lsh_compare(candidates, corpus_mh, jaccard_similarity)
as.data.frame(comps)[order(as.data.frame(comps)[, 3], decreasing = TRUE),]
```{r}
id1 = 982
id2 = 991
d[c(id1, id2), 2]
Text1 = TextReuseTextDocument(text = as.character(d[id1, 4]), meta = list("id" = "doc-1000"), tokenizer = tokenize_ngrams, n = 3)
Text2 = TextReuseTextDocument(text = as.character(d[id2, 4]), meta = list("id" = "doc-136"), tokenizer = tokenize_ngrams, n = 3)
align_local(Text1, Text2)
id1 = 1204
id2 = 4
d[c(id1, id2), 2]
Text1 = TextReuseTextDocument(text = as.character(d[id1, 4]), meta = list("id" = "doc-1000"), tokenizer = tokenize_ngrams, n = 3)
Text2 = TextReuseTextDocument(text = as.character(d[id2, 4]), meta = list("id" = "doc-136"), tokenizer = tokenize_ngrams, n = 3)
align_local(Text1, Text2)
id1 = 427
id2 = 533
d[c(id1, id2), 2]
Text1 = TextReuseTextDocument(text = as.character(d[id1, 4]), meta = list("id" = "doc-1000"), tokenizer = tokenize_ngrams, n = 3)
Text2 = TextReuseTextDocument(text = as.character(d[id2, 4]), meta = list("id" = "doc-136"), tokenizer = tokenize_ngrams, n = 3)
align_local(Text1, Text2)
install.packages('FactoMineR')
library(FactoMineR)
setwd("~/Documents/ProjetEchevinsDeStrasbourg/EchevinsDB/exports")
read.table('df_cumul.csv', row.names=1)
read.table('df_cumul.csv')
read.csv(file="df_cumul.csv")
df1<- data.frame(df[,-1], row.names = df1[,1])
df <- read.csv(file="df_cumul.csv")
df1 <- data.frame(df[,-1], row.names = df1[,1])
df1 <- data.frame(df[,-1], row.names = df[,1])
View(df1)
library(ggridges)
install.packages('ggrides')
library(ggridges)
install.packages('ggridges')
library(ggridges)
library(ggplot2)
